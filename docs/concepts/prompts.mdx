---
title: "Prompts"
description: "Create resuable prompt templates and workflows"
---

MCP Prompts enable servers to define reusable prompt templates and workflows that clients can easily surface to users and LLMs. They provide a powerful way to standardize and share common LLM interactions.

## Overview

Prompts in MCP are predefined templates that can:
- Accept dynamic arguments
- Include context from resources
- Chain multiple interactions
- Guide specific workflows
- Surface as UI elements (like slash commands)

## Prompt structure

Each prompt is defined with:

```typescript
{
  name: string;           // Unique identifier for the prompt
  description?: string;   // Human-readable description
  arguments?: [          // Optional list of arguments
    {
      name: string;      // Argument identifier
      description?: string; // Argument description
      required?: boolean;  // Whether argument is required
    }
  ]
}
```

## Discovering prompts

Clients can discover available prompts through the `prompts/list` endpoint:

```typescript
// Request
{
  method: "prompts/list"
}

// Response
{
  prompts: [
    {
      name: "analyze-code",
      description: "Analyze code for potential improvements",
      arguments: [
        {
          name: "language",
          description: "Programming language",
          required: true
        }
      ]
    }
  ]
}
```

## Using prompts

To use a prompt, clients make a `prompts/get` request:

```typescript
// Request
{
  method: "prompts/get",
  params: {
    name: "analyze-code",
    arguments: {
      language: "python"
    }
  }
}

// Response
{
  description?: string,  // Optional context about the prompt
  messages: [           // Messages to send to the LLM
    {
      role: "user" | "assistant",
      content: {
        type: "text" | "image",
        text?: string,
        data?: string,  // base64 for images
        mimeType?: string
      }
    }
  ]
}
```

## Dynamic prompts

Prompts can be dynamic and include:

### Context Integration

```typescript
const analyzeLogsPrompt = {
  name: "analyze-logs",
  description: "Analyze recent system logs",
  arguments: [
    {
      name: "timeframe",
      description: "Time period to analyze",
      required: true
    }
  ]
};

server.setRequestHandler(GetPromptRequestSchema, async (request) => {
  if (request.params.name === "analyze-logs") {
    const logs = await getSystemLogs(request.params.arguments?.timeframe);
    return {
      messages: [
        {
          role: "user",
          content: {
            type: "text",
            text: `Analyze these system logs and identify any issues:\n\n${logs}`
          }
        }
      ]
    };
  }
});
```

### Resource references

```typescript
const codeReviewPrompt = {
  name: "review-code",
  async getMessages(fileUri: string) {
    const code = await readResource(fileUri);
    return [
      {
        role: "user",
        content: {
          type: "text",
          text: `Review this code for best practices:\n\n${code}`
        }
      }
    ];
  }
};
```

### Multi-step workflows

```typescript
const debugWorkflow = {
  name: "debug-error",
  async getMessages(error: string) {
    return [
      {
        role: "user",
        content: {
          type: "text",
          text: "Here's an error I'm seeing:"
        }
      },
      {
        role: "assistant",
        content: {
          type: "text",
          text: "I'll help analyze this error. What have you tried so far?"
        }
      },
      {
        role: "user",
        content: {
          type: "text",
          text: error
        }
      }
    ];
  }
};
```

## Example implementation

Here's a complete example of implementing prompts in an MCP server:

```typescript
import { Server } from "@modelcontextprotocol/sdk/server";
import { 
  ListPromptsRequestSchema,
  GetPromptRequestSchema
} from "@modelcontextprotocol/sdk/types";

const PROMPTS = {
  "git-commit": {
    name: "git-commit",
    description: "Generate a Git commit message",
    arguments: [
      {
        name: "changes",
        description: "Git diff or description of changes",
        required: true
      }
    ]
  },
  "explain-code": {
    name: "explain-code",
    description: "Explain how code works",
    arguments: [
      {
        name: "code",
        description: "Code to explain",
        required: true
      },
      {
        name: "language",
        description: "Programming language",
        required: false
      }
    ]
  }
};

const server = new Server({
  name: "example-prompts-server",
  version: "1.0.0"
});

// List available prompts
server.setRequestHandler(ListPromptsRequestSchema, async () => {
  return {
    prompts: Object.values(PROMPTS)
  };
});

// Get specific prompt
server.setRequestHandler(GetPromptRequestSchema, async (request) => {
  const prompt = PROMPTS[request.params.name];
  if (!prompt) {
    throw new Error(`Prompt not found: ${request.params.name}`);
  }

  if (request.params.name === "git-commit") {
    return {
      messages: [
        {
          role: "user",
          content: {
            type: "text",
            text: `Generate a concise but descriptive commit message for these changes:\n\n${request.params.arguments?.changes}`
          }
        }
      ]
    };
  }

  if (request.params.name === "explain-code") {
    const language = request.params.arguments?.language || "Unknown";
    return {
      messages: [
        {
          role: "user",
          content: {
            type: "text",
            text: `Explain how this ${language} code works:\n\n${request.params.arguments?.code}`
          }
        }
      ]
    };
  }
  
  throw new Error("Prompt implementation not found");
});
```

## Best practices

When implementing prompts:

1. Use clear, descriptive prompt names
2. Provide detailed descriptions for prompts and arguments
3. Validate all required arguments
4. Handle missing arguments gracefully
5. Consider versioning for prompt templates
6. Cache dynamic content when appropriate
7. Implement error handling
8. Document expected argument formats
9. Consider prompt composability
10. Test prompts with various inputs

## UI integration

Prompts can be surfaced in client UIs as:

- Slash commands
- Quick actions
- Context menu items
- Command palette entries
- Guided workflows
- Interactive forms

## Updates and changes

Servers can notify clients about prompt changes:

1. Server capability: `prompts.listChanged`
2. Notification: `notifications/prompts/list_changed`
3. Client re-fetches prompt list

## Security considerations

When implementing prompts:

- Validate all arguments
- Sanitize user input
- Consider rate limiting
- Implement access controls
- Audit prompt usage
- Handle sensitive data appropriately
- Validate generated content
- Implement timeouts
- Consider prompt injection risks
- Document security requirements

## Advanced features

### Prompt chaining

```typescript
const analyzePRPrompt = {
  async getMessages(prId: string) {
    // Chain multiple prompts
    const diffPrompt = await getPrompt("analyze-diff");
    const testsPrompt = await getPrompt("review-tests");
    const securityPrompt = await getPrompt("security-review");
    
    return [
      ...diffPrompt.messages,
      ...testsPrompt.messages,
      ...securityPrompt.messages
    ];
  }
};
```

### Context enrichment

```typescript
const troubleshootPrompt = {
  async getMessages(error: string) {
    // Enrich with system context
    const systemInfo = await getSystemInfo();
    const recentLogs = await getRecentLogs();
    const stackTrace = await parseError(error);
    
    return [
      {
        role: "user",
        content: {
          type: "text",
          text: `Help troubleshoot this error:
Error: ${error}
System: ${systemInfo}
Logs: ${recentLogs}
Stack: ${stackTrace}`
        }
      }
    ];
  }
};
```

### Interactive workflows

```typescript
const debugWorkflow = {
  async getMessages(stage: string, context: any) {
    switch (stage) {
      case "initial":
        return promptForError();
      case "analysis":
        return analyzeError(context.error);
      case "solution":
        return suggestFixes(context.analysis);
      case "verification":
        return verifyFix(context.solution);
    }
  }
};
```